{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=4, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Linear(in_features=4, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class experiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(experiment, self).__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(3,4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc2 = nn.Linear(4,3)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = experiment()\n",
    "net2 = experiment()\n",
    "net3 = experiment()\n",
    "\n",
    "for params in net.parameters():\n",
    "    nn.init.normal_(params, mean=0, std=0.01)\n",
    "    \n",
    "net2.load_state_dict(net.state_dict())\n",
    "net3.load_state_dict(net.state_dict())\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1 fc2.weight\n",
      "tensor([[ 0.0118, -0.0146, -0.0047,  0.0008],\n",
      "        [-0.0121, -0.0112,  0.0092, -0.0127],\n",
      "        [-0.0025,  0.0031, -0.0149, -0.0013]])\n",
      "net2 fc2.weight\n",
      "tensor([[ 0.0118, -0.0146, -0.0047,  0.0008],\n",
      "        [-0.0121, -0.0112,  0.0092, -0.0127],\n",
      "        [-0.0025,  0.0031, -0.0149, -0.0013]])\n",
      "net3 fc2.weight\n",
      "tensor([[ 0.0118, -0.0146, -0.0047,  0.0008],\n",
      "        [-0.0121, -0.0112,  0.0092, -0.0127],\n",
      "        [-0.0025,  0.0031, -0.0149, -0.0013]])\n"
     ]
    }
   ],
   "source": [
    "print('net1 fc2.weight')\n",
    "print(net.state_dict()['fc2.weight'])\n",
    "print('net2 fc2.weight')\n",
    "print(net2.state_dict()['fc2.weight'])\n",
    "print('net3 fc2.weight')\n",
    "print(net3.state_dict()['fc2.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "# loss_func2 = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=0.1)\n",
    "optimizer3 = optim.SGD(net3.parameters(), lr=0.1)\n",
    "def my_cross_entropy(y, t):\n",
    "    delta = 1e-7\n",
    "#     print(y.size(0))\n",
    "#     print(\"这是 my_cross_entropy 中传入的 y\")\n",
    "#     print(y)\n",
    "#     print(\"这是 my_cross_entropy 中传入的 y，经过log处理\")\n",
    "#     print(torch.log(y))\n",
    "    return -torch.sum(t * torch.log(y + delta)) / y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_rule(net, loss_func, optimizer, data, labels, PI):\n",
    "    student_outputs = net(data) \n",
    "#     print(\"this is student_outputs\")\n",
    "#     print(student_outputs)\n",
    "    student_loss = loss_func(student_outputs, labels)\n",
    "    print(\"这是student loss\")\n",
    "    print(student_loss)\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    student_outputs_softmax = softmax(student_outputs)\n",
    "#     print(\"this is softmax student_outputs\")\n",
    "#     print(student_outputs_softmax)\n",
    "    teacher_q = student_outputs_softmax * math.exp(0.5)\n",
    "#    print(teacher_q)\n",
    "    teacher_loss = my_cross_entropy(student_outputs_softmax, teacher_q)\n",
    "    print(\"这是teacher loss\")\n",
    "    print(teacher_loss)\n",
    "#     teacher_loss = loss_func2(student_outputs, student_outputs*math.exp(0.5))\n",
    "    loss = (1-PI) * student_loss + PI * teacher_loss\n",
    "    print(\"这是联合 loss\")\n",
    "    print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(net.state_dict()['fc2.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_norule(net, loss_func, optimizer, data, labels):\n",
    "    student_outputs = net(data)    \n",
    "    loss = loss_func(student_outputs, labels) # input(N,C) where C= number of classes Target(N)\n",
    "    print(\"这是 loss\")\n",
    "    print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(net.state_dict()['fc2.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是student loss\n",
      "tensor(1.0991, grad_fn=<NllLossBackward>)\n",
      "这是teacher loss\n",
      "tensor(1.8113, grad_fn=<DivBackward0>)\n",
      "这是联合 loss\n",
      "tensor(1.6689, grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.0118, -0.0144, -0.0047,  0.0009],\n",
      "        [-0.0122, -0.0112,  0.0092, -0.0127],\n",
      "        [-0.0024,  0.0030, -0.0149, -0.0013]])\n"
     ]
    }
   ],
   "source": [
    "x_input = torch.randn(5, 3)\n",
    "# print(x_input)\n",
    "y_target = torch.empty(5, dtype=torch.long).random_(3)\n",
    "train_with_rule(net, loss_func, optimizer, x_input, y_target, 0.8)\n",
    "# print('net3 fc2.weight')\n",
    "# train_with_norule(net3, loss_func, optimizer3, x_input, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net2 fc2.weight\n",
      "这是 loss\n",
      "tensor(1.0991, grad_fn=<NllLossBackward>)\n",
      "tensor([[ 0.0119, -0.0140, -0.0044,  0.0010],\n",
      "        [-0.0126, -0.0115,  0.0091, -0.0128],\n",
      "        [-0.0021,  0.0028, -0.0150, -0.0014]])\n"
     ]
    }
   ],
   "source": [
    "print('net2 fc2.weight')\n",
    "train_with_norule(net2, loss_func, optimizer2, x_input, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net3 fc2.weight\n",
      "这是 loss\n",
      "tensor(1.0991, grad_fn=<NllLossBackward>)\n",
      "tensor([[ 0.0119, -0.0140, -0.0044,  0.0010],\n",
      "        [-0.0126, -0.0115,  0.0091, -0.0128],\n",
      "        [-0.0021,  0.0028, -0.0150, -0.0014]])\n"
     ]
    }
   ],
   "source": [
    "print('net3 fc2.weight')\n",
    "train_with_norule(net3, loss_func, optimizer3, x_input, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
